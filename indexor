#!/usr/bin/env python3
"""
indexed-find: Fast keyword-based file search with boolean and sequential queries
"""

import sqlite3
import re
import os
import sys
import time
import threading
import queue
import psutil
from pathlib import Path
from typing import List, Tuple, Set, Optional
import argparse

class BooleanParser:
    """Parse and evaluate boolean expressions"""
    
    def __init__(self, keywords: Set[str]):
        self.keywords = keywords
    
    def parse(self, expr: str) -> bool:
        """Parse boolean expression with &&, ||, (), and ! operators"""
        expr = expr.strip()
        
        # Handle parentheses recursively
        while '(' in expr:
            # Find innermost parentheses
            start = expr.rfind('(')
            end = expr.find(')', start)
            if end == -1:
                raise ValueError("Mismatched parentheses")
            
            inner = expr[start+1:end]
            result = self.parse(inner)
            expr = expr[:start] + str(result) + expr[end+1:]
        
        # Handle OR (lower precedence)
        if '||' in expr:
            parts = expr.split('||')
            return any(self.parse(p.strip()) for p in parts)
        
        # Handle AND (higher precedence)
        if '&&' in expr:
            parts = expr.split('&&')
            return all(self.parse(p.strip()) for p in parts)
        
        # Handle NOT
        if expr.startswith('!'):
            return not self.parse(expr[1:].strip())
        
        # Base case: check if keyword exists
        if expr in ('True', 'False'):
            return expr == 'True'
        
        return expr.strip().lower() in self.keywords


class Indexer:
    """Index files for fast keyword search"""
    
    def __init__(self, db_path: str, commit_time: float = 0, threads: int = 1, max_readahead_pct: float = 10):
        self.db_path = db_path
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self.create_tables()
        self.stats = {'total': 0, 'indexed': 0, 'skipped': 0}
        self.verbosity = 0
        self.commit_time = commit_time
        self.last_commit = time.time()
        self.threads = threads
        self.max_readahead_bytes = int(psutil.virtual_memory().total * (max_readahead_pct / 100))
        self.stats_lock = threading.Lock()

        self.conn.execute('PRAGMA journal_mode=WAL')  # CRITICAL for SMR
        self.conn.execute('PRAGMA synchronous=NORMAL')  # Faster, still safe
        self.conn.execute('PRAGMA cache_size=-4000000')  # 4GB cache (negative = KB)
        self.conn.execute('PRAGMA temp_store=MEMORY')
        self.conn.execute('PRAGMA mmap_size=268435456')  # 256MB memory-mapped I/O

    
    def create_tables(self):
        """Create database schema"""
        self.conn.execute('PRAGMA page_size=65536')  # Larger pages (default is 4096)
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS keywords (
                id INTEGER PRIMARY KEY,
                keyword TEXT NOT NULL,
                filename TEXT NOT NULL,
                offset INTEGER NOT NULL,
                prior_line TEXT,
                match_line TEXT,
                next_line TEXT
            )
        ''')
        self.conn.execute('CREATE INDEX IF NOT EXISTS idx_keyword ON keywords(keyword)')
        self.conn.execute('CREATE INDEX IF NOT EXISTS idx_filename ON keywords(filename)')
        self.conn.commit()
    
    def extract_keywords(self, text: str) -> Set[str]:
        """Extract keywords using both patterns"""
        keywords = set()
        
        # Pattern 1: [a-z0-9]+
        for match in re.finditer(r'[a-z0-9]+', text, re.IGNORECASE):
            kw = match.group(0).lower()
            if len(kw) <= 40:
                keywords.add(kw)
        
        # Pattern 2: [a-z0-9_]+
        for match in re.finditer(r'[a-z0-9_]+', text, re.IGNORECASE):
            kw = match.group(0).lower()
            if len(kw) <= 40:
                keywords.add(kw)
        
        return keywords
    
    def get_context(self, content: bytes, offset: int) -> Tuple[str, str, str]:
        """Extract prior, current, and next line with 160 char limits"""
        try:
            text = content.decode('utf-8', errors='ignore')
        except:
            return '', '', ''
        
        # Find line boundaries
        line_start = text.rfind('\n', 0, offset)
        if line_start == -1:
            line_start = 0
        else:
            line_start += 1
        
        line_end = text.find('\n', offset)
        if line_end == -1:
            line_end = len(text)
        
        current_line = text[line_start:line_end]
        
        # Prior line
        prior_line = ''
        if line_start > 0:
            prior_start = text.rfind('\n', 0, line_start - 1)
            if prior_start == -1:
                prior_start = 0
            else:
                prior_start += 1
            
            prior_text = text[prior_start:line_start - 1]
            if len(prior_text) > 160:
                prior_line = prior_text[-160:]
            else:
                prior_line = prior_text
        
        # Next line
        next_line = ''
        if line_end < len(text) - 1:
            next_start = line_end + 1
            next_end = text.find('\n', next_start)
            if next_end == -1:
                next_end = len(text)
            
            next_text = text[next_start:next_end]
            if len(next_text) > 160:
                next_line = next_text[:160]
            else:
                next_line = next_text
        
        # Limit current line
        if len(current_line) > 160:
            # Find where match is in current line
            rel_offset = offset - line_start
            if rel_offset < 160:
                current_line = current_line[:160]
            else:
                current_line = current_line[rel_offset-80:rel_offset+80]
        
        return prior_line, current_line, next_line
    
    def maybe_commit(self):
        """Commit if enough time has passed since last commit"""
        now = time.time()
        if self.commit_time == 0 or (now - self.last_commit) >= self.commit_time:
            self.conn.commit()
            self.last_commit = now
    
    def is_indexed(self, filepath: str) -> bool:
        """Check if file is already indexed"""
        cursor = self.conn.execute('''
            SELECT COUNT(*) FROM keywords WHERE filename = ?
        ''', (filepath,))
        return cursor.fetchone()[0] > 0
    
    def index_directory(self, directory: str, maxsize: int = 200000, append: bool = False, recursive: bool = False, maxindexed: int = None):
        """Index files in directory (non-recursive by default, processes one at a time)"""
        if self.threads == 1:
            self._index_directory_single_threaded(directory, maxsize, append, recursive, maxindexed)
        else:
            self._index_directory_multi_threaded(directory, maxsize, append, recursive, maxindexed)
    
    def _index_directory_single_threaded(self, directory: str, maxsize: int, append: bool, recursive: bool, maxindexed: int):
        """Original single-threaded implementation"""
        base_dir = os.path.abspath(directory)
        
        if recursive:
            for root, dirs, files in os.walk(base_dir):
                for filename in files:
                    if maxindexed and self.stats['indexed'] >= maxindexed:
                        print(f"\nReached max indexed limit: {maxindexed}")
                        self.conn.commit()
                        return
                    filepath = os.path.join(root, filename)
                    relative_path = os.path.relpath(filepath, base_dir)
                    self._index_file_if_valid(filepath, relative_path, maxsize, append)
        else:
            try:
                entries = os.listdir(base_dir)
            except OSError as e:
                print(f"Error reading directory {base_dir}: {e}", file=sys.stderr)
                return
            
            for entry in entries:
                if maxindexed and self.stats['indexed'] >= maxindexed:
                    print(f"\nReached max indexed limit: {maxindexed}")
                    self.conn.commit()
                    return
                filepath = os.path.join(base_dir, entry)
                if os.path.isfile(filepath):
                    self._index_file_if_valid(filepath, entry, maxsize, append)
        
        self.conn.commit()
        if self.verbosity == 0:
            print(f"\rTotal:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']}")
        else:
            print(f"\rTotal:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']}")
    
    def _index_directory_multi_threaded(self, directory: str, maxsize: int, append: bool, recursive: bool, maxindexed: int):
        """Multi-threaded implementation with reader, workers, and writer"""
        base_dir = os.path.abspath(directory)
        
        # Queues
        work_queue = queue.Queue(maxsize=100)
        write_queue = queue.Queue(maxsize=5000)
        
        # Shared state
        reader_done = threading.Event()
        workers_done = threading.Event()
        shutdown = threading.Event()
        queue_bytes_lock = threading.Lock()
        queued_bytes = [0]
        
        # Reader thread
        def reader():
            try:
                file_list = []
                if recursive:
                    for root, dirs, files in os.walk(base_dir):
                        for filename in files:
                            filepath = os.path.join(root, filename)
                            relative_path = os.path.relpath(filepath, base_dir)
                            file_list.append((filepath, relative_path))
                else:
                    try:
                        entries = os.listdir(base_dir)
                        for entry in entries:
                            filepath = os.path.join(base_dir, entry)
                            if os.path.isfile(filepath):
                                file_list.append((filepath, entry))
                    except OSError as e:
                        print(f"Error reading directory: {e}", file=sys.stderr)
                        return
                
                for filepath, relative_path in file_list:
                    if shutdown.is_set():
                        break
                    
                    with self.stats_lock:
                        if maxindexed and self.stats['indexed'] >= maxindexed:
                            break
                    
                    try:
                        file_size = os.path.getsize(filepath)
                    except OSError:
                        with self.stats_lock:
                            self.stats['total'] += 1
                            self.stats['skipped'] += 1
                        continue
                    
                    if file_size == 0 or file_size > maxsize:
                        with self.stats_lock:
                            self.stats['total'] += 1
                            self.stats['skipped'] += 1
                        if self.verbosity >= 2 and file_size > maxsize:
                            print(f"Skipping {relative_path} (size: {file_size} > {maxsize})")
                        continue
                    
                    if append and self.is_indexed(relative_path):
                        with self.stats_lock:
                            self.stats['total'] += 1
                            self.stats['skipped'] += 1
                        if self.verbosity >= 2:
                            print(f"Skipping {relative_path} (already indexed)")
                        continue
                    
                    # Memory limit check
                    with queue_bytes_lock:
                        while queued_bytes[0] > self.max_readahead_bytes and queued_bytes[0] > 0:
                            queue_bytes_lock.release()
                            time.sleep(0.01)
                            queue_bytes_lock.acquire()
                    
                    try:
                        with open(filepath, 'rb') as f:
                            content = f.read()
                        
                        with queue_bytes_lock:
                            queued_bytes[0] += len(content)
                        
                        work_queue.put((filepath, content, relative_path, len(content)))
                        
                        with self.stats_lock:
                            self.stats['total'] += 1
                            if self.verbosity >= 1:
                                print(f"\rTotal:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']} Reading: {relative_path} ({file_size} bytes)")
                        
                        if self.verbosity >= 1:
                            print(f"Reading: {relative_path} ({file_size} bytes)")
                            
                    except Exception as e:
                        print(f"Error reading {filepath}: {e}", file=sys.stderr)
                        with self.stats_lock:
                            self.stats['total'] += 1
                            self.stats['skipped'] += 1
            
            finally:
                reader_done.set()
        
        # Worker threads
        def worker():
            while not (reader_done.is_set() and work_queue.empty()):
                try:
                    filepath, content, relative_path, file_bytes = work_queue.get(timeout=0.1)
                    
                    try:
                        text = content.decode('utf-8', errors='ignore')
                        
                        if self.verbosity >= 2:
                            print(f"Indexing: {relative_path}")
                        
                        for pattern in [r'[a-z0-9]+', r'[a-z0-9_]+']:
                            for match in re.finditer(pattern, text, re.IGNORECASE):
                                keyword = match.group(0).lower()
                                if len(keyword) <= 40:
                                    offset = match.start()
                                    prior, current, next_line = self.get_context(content, offset)
                                    
                                    write_queue.put((keyword, relative_path, offset, prior, current, next_line))
                        
                        with self.stats_lock:
                            self.stats['indexed'] += 1
                            if self.verbosity >= 1:
                                print(f"\rTotal:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']} Indexed: {relative_path}")
                    
                    except Exception as e:
                        print(f"Error processing {filepath}: {e}", file=sys.stderr)
                        with self.stats_lock:
                            self.stats['skipped'] += 1
                    
                    finally:
                        with queue_bytes_lock:
                            queued_bytes[0] -= file_bytes
                        work_queue.task_done()
                
                except queue.Empty:
                    continue
            
            workers_done.set()
        
        # Writer thread
        def writer():
            batch = []
            last_flush = time.time()
            
            while not (workers_done.is_set() and write_queue.empty()):
                try:
                    item = write_queue.get(timeout=0.1)
                    batch.append(item)
                    
                    now = time.time()
                    if len(batch) >= 100000 or (self.commit_time > 0 and (now - last_flush) >= self.commit_time):
                        if self.verbosity >= 2:
                            print(f"Storing Index: {len(batch)} keywords")
                        self.conn.executemany('''
                            INSERT INTO keywords (keyword, filename, offset, prior_line, match_line, next_line)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', batch)
                        self.conn.commit()
                        batch = []
                        last_flush = now
                    
                    write_queue.task_done()
                
                except queue.Empty:
                    if batch:
                        now = time.time()
                        if self.commit_time == 0 or (now - last_flush) >= self.commit_time:
                            if self.verbosity >= 2:
                                print(f"Storing Index: {len(batch)} keywords")
                            self.conn.executemany('''
                                INSERT INTO keywords (keyword, filename, offset, prior_line, match_line, next_line)
                                VALUES (?, ?, ?, ?, ?, ?)
                            ''', batch)
                            self.conn.commit()
                            batch = []
                            last_flush = now
            
            if batch:
                if self.verbosity >= 2:
                    print(f"Storing Index: {len(batch)} keywords (final)")
                self.conn.executemany('''
                    INSERT INTO keywords (keyword, filename, offset, prior_line, match_line, next_line)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', batch)
                self.conn.commit()
        
        # Start threads
        reader_thread = threading.Thread(target=reader, name="Reader")
        worker_threads = [threading.Thread(target=worker, name=f"Worker-{i}") 
                         for i in range(max(1, self.threads - 2))]
        writer_thread = threading.Thread(target=writer, name="Writer")
        
        reader_thread.start()
        for t in worker_threads:
            t.start()
        writer_thread.start()
        
        start_time = time.time()
        try:
            while reader_thread.is_alive() or any(t.is_alive() for t in worker_threads) or writer_thread.is_alive():
                if self.verbosity == 0:
                    with self.stats_lock:
                        print(f"\rTotal:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']}", 
                              end='', flush=True)
                time.sleep(0.5)
        except KeyboardInterrupt:
            print("\nShutdown requested, stopping threads...")
            shutdown.set()
            reader_thread.join(timeout=2)
            for t in worker_threads:
                t.join(timeout=2)
            writer_thread.join(timeout=2)
            sys.exit(1)
        
        reader_thread.join()
        for t in worker_threads:
            t.join()
        writer_thread.join()
        
        elapsed = time.time() - start_time
        if self.verbosity == 0:
            print(f"\rTotal:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']} (took {elapsed:.1f}s)")
        else:
            print(f"Total:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']} (took {elapsed:.1f}s)")
    
    def _index_file_if_valid(self, filepath: str, relative_path: str, maxsize: int, append: bool):
        """Helper to check and index a single file"""
        self.stats['total'] += 1
        
        if self.verbosity == 0:
            print(f"\rTotal:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']}", end='', flush=True)
        else:
            print(f"Total:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']}", end=' ', flush=True)
        
        try:
            file_size = os.path.getsize(filepath)
        except OSError:
            self.stats['skipped'] += 1
            if self.verbosity >= 2:
                print(f"Skipping {relative_path} (error reading)")
            elif self.verbosity >= 1:
                print()
            return
        
        if file_size == 0:
            self.stats['skipped'] += 1
            if self.verbosity >= 2:
                print(f"Skipping {relative_path} (0 bytes)")
            elif self.verbosity >= 1:
                print()
            return
        
        if file_size > maxsize:
            self.stats['skipped'] += 1
            if self.verbosity >= 2:
                print(f"Skipping {relative_path} (size: {file_size} > {maxsize})")
            elif self.verbosity >= 1:
                print()
            return
        
        if append and self.is_indexed(relative_path):
            self.stats['skipped'] += 1
            if self.verbosity >= 2:
                print(f"Skipping {relative_path} (already indexed)")
            elif self.verbosity >= 1:
                print()
            return
        
        if self.verbosity >= 1:
            print(f"Indexing: {relative_path} ({file_size} bytes)")
        
        self.stats['indexed'] += 1
        self.index_file_with_name(filepath, relative_path)
    
    def index_file(self, filepath: str):
        """Index a single file (deprecated - use index_file_with_name)"""
        self.index_file_with_name(filepath, filepath)
    
    def index_file_with_name(self, filepath: str, stored_name: str):
        """Index a file but store it with the given name"""
        try:
            with open(filepath, 'rb') as f:
                content = f.read()
            
            text = content.decode('utf-8', errors='ignore')
            
            for pattern in [r'[a-z0-9]+', r'[a-z0-9_]+']:
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    keyword = match.group(0).lower()
                    if len(keyword) <= 40:
                        offset = match.start()
                        prior, current, next_line = self.get_context(content, offset)
                        
                        self.conn.execute('''
                            INSERT INTO keywords (keyword, filename, offset, prior_line, match_line, next_line)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (keyword, stored_name, offset, prior, current, next_line))
            
            self.maybe_commit()
        except Exception as e:
            print(f"Error indexing {filepath}: {e}", file=sys.stderr)


class Searcher:
    """Search indexed files"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.HIGHLIGHT = '\033[35;1m'
        self.RESET = '\033[0m'
    
    def highlight_keywords(self, text: str, keywords: List[str]) -> str:
        """Highlight keywords in text"""
        if not text:
            return text
        
        result = text
        sorted_keywords = sorted(keywords, key=len, reverse=True)
        
        for keyword in sorted_keywords:
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            result = pattern.sub(f'{self.HIGHLIGHT}\\g<0>{self.RESET}', result)
        
        return result
    
    def boolean_search(self, query: str, debug: bool = False) -> List[Tuple]:
        """Search using boolean logic"""
        potential_keywords = re.findall(r'\w+', query)
        keywords_to_check = [kw.lower() for kw in potential_keywords 
                            if kw.lower() not in ('and', 'or', 'not')]
        
        if debug:
            print(f"Debug: Searching for keywords: {keywords_to_check}", file=sys.stderr)
        
        if not keywords_to_check:
            return []
        
        placeholders = ','.join('?' * len(keywords_to_check))
        cursor = self.conn.execute(f'''
            SELECT DISTINCT filename FROM keywords WHERE keyword IN ({placeholders})
        ''', keywords_to_check)
        
        files = [row[0] for row in cursor.fetchall()]
        
        if debug:
            print(f"Debug: Found {len(files)} files with at least one keyword", file=sys.stderr)
        
        results = []
        for filename in files:
            cursor = self.conn.execute('''
                SELECT DISTINCT keyword FROM keywords WHERE filename = ?
            ''', (filename,))
            file_keywords = set(row[0] for row in cursor.fetchall())
            
            if debug:
                matching = [kw for kw in keywords_to_check if kw in file_keywords]
                print(f"Debug: {filename} has keywords: {matching}", file=sys.stderr)
            
            parser = BooleanParser(file_keywords)
            try:
                if parser.parse(query):
                    if debug:
                        print(f"Debug: {filename} MATCHES query", file=sys.stderr)
                    
                    positive_keywords = [kw for kw in keywords_to_check if not query.count(f'!{kw}')]
                    if not positive_keywords:
                        positive_keywords = keywords_to_check
                    
                    placeholders_pos = ','.join('?' * len(positive_keywords))
                    cursor = self.conn.execute(f'''
                        SELECT keyword, offset, prior_line, match_line, next_line
                        FROM keywords WHERE filename = ? AND keyword IN ({placeholders_pos})
                        LIMIT 3
                    ''', (filename,) + tuple(positive_keywords))
                    
                    for row in cursor.fetchall():
                        results.append((filename,) + row)
            except Exception as e:
                print(f"Error parsing query: {e}", file=sys.stderr)
                return []
        
        return results
    
    def sequential_search(self, query: str, debug: bool = False) -> List[Tuple]:
        """Search for keywords in order using space-separated or < operator"""
        keywords = [kw.strip().lower() for kw in query.split() if kw.strip()]
        
        if debug:
            print(f"Debug: Sequential search for: {keywords}", file=sys.stderr)
        
        if len(keywords) < 2:
            return []
        
        placeholders = ','.join('?' * len(keywords))
        cursor = self.conn.execute(f'''
            SELECT filename FROM keywords WHERE keyword IN ({placeholders})
            GROUP BY filename HAVING COUNT(DISTINCT keyword) = ?
        ''', keywords + [len(keywords)])
        
        files = [row[0] for row in cursor.fetchall()]
        
        if debug:
            print(f"Debug: Found {len(files)} files with all keywords", file=sys.stderr)
        
        results = []
        for filename in files:
            cursor = self.conn.execute(f'''
                SELECT keyword, offset, prior_line, match_line, next_line
                FROM keywords WHERE filename = ? AND keyword IN ({placeholders})
                ORDER BY offset
            ''', (filename,) + tuple(keywords))
            
            occurrences = cursor.fetchall()
            
            keyword_positions = {kw: [] for kw in keywords}
            for kw, offset, *context in occurrences:
                keyword_positions[kw].append((offset, context))
            
            if self.check_sequence(keyword_positions, keywords):
                if debug:
                    print(f"Debug: {filename} has keywords in correct sequence", file=sys.stderr)
                
                for kw in keywords:
                    if keyword_positions[kw]:
                        offset, context = keyword_positions[kw][0]
                        results.append((filename, kw, offset) + tuple(context))
        
        return results
    
    def check_sequence(self, positions: dict, keywords: List[str]) -> bool:
        """Check if keywords appear in sequence"""
        for first_offset, _ in positions[keywords[0]]:
            last_offset = first_offset
            valid = True
            
            for kw in keywords[1:]:
                found = False
                for offset, _ in positions[kw]:
                    if offset > last_offset:
                        last_offset = offset
                        found = True
                        break
                
                if not found:
                    valid = False
                    break
            
            if valid:
                return True
        
        return False


def main():
    DEF_MAXSIZE = 200000
    DEF_COMMIT_TIME = 0
    DEF_THREADS = 1
    DEF_READAHEAD_PCT = 10
    
    parser = argparse.ArgumentParser(description='Index and search files for recovery')
    parser.add_argument('-n', '--dataset-name', required=True, help='Dataset name')
    parser.add_argument('command', nargs='?', choices=['index', 'search'], help='Command to run')
    parser.add_argument('query', nargs='*', help='Search query (multiple args for sequential search)')
    parser.add_argument('-d', '--dir', help='Directory to index (required for index command)')
    parser.add_argument('-r', '--recursive', action='store_true', help='Recursively index subdirectories')
    parser.add_argument('-a', '--append', action='store_true', help='Append to existing index')
    parser.add_argument('--overwrite', action='store_true', help='Overwrite existing index (requires --force)')
    parser.add_argument('--force', action='store_true', help='Force overwrite (requires --overwrite)')
    parser.add_argument('--maxsize', type=int, default=DEF_MAXSIZE, 
                       help=f'Maximum file size in bytes (default: {DEF_MAXSIZE})')
    parser.add_argument('-v', '--verbose', action='count', default=0,
                       help='Increase verbosity (-v, -v -v, or -v -v -v)')
    parser.add_argument('--maxindexed', type=int, default=None,
                       help='Stop after indexing N files (for testing)')
    parser.add_argument('--debug', action='store_true',
                       help='Show debug information for searches')
    parser.add_argument('-l', '--list-files', action='store_true',
                       help='List all files in the index')
    parser.add_argument('-k', '--list-keywords', action='store_true',
                       help='List all keywords in the index')
    parser.add_argument('--commit-time', '--ct', type=float, default=DEF_COMMIT_TIME,
                       help=f'Minimum time between commits to sqlite database. Default ({DEF_COMMIT_TIME}) = Write after each file is indexed.')
    parser.add_argument('--threads', type=int, default=DEF_THREADS,
                       help=f'Number of threads (default: {DEF_THREADS} = single-threaded). N > 1: Uses 1 reader + (N-1) workers + 1 writer')
    parser.add_argument('--max-readahead-sysmem', type=float, default=DEF_READAHEAD_PCT,
                       help=f'Percentage of system memory for file readahead queue (default: {DEF_READAHEAD_PCT}%%)')
    
    args = parser.parse_args()
    
    db_path = os.path.expanduser(f'~/.cache/indexor/ds/{args.dataset_name}/index.db')
    
    if args.list_files:
        if not os.path.exists(db_path):
            print(f"Index {args.dataset_name} does not exist")
            sys.exit(1)
        conn = sqlite3.connect(db_path)
        cursor = conn.execute('SELECT DISTINCT filename FROM keywords ORDER BY filename')
        for row in cursor.fetchall():
            print(row[0])
        conn.close()
        return
    
    if args.list_keywords:
        if not os.path.exists(db_path):
            print(f"Index {args.dataset_name} does not exist")
            sys.exit(1)
        conn = sqlite3.connect(db_path)
        cursor = conn.execute('SELECT DISTINCT keyword FROM keywords ORDER BY keyword')
        for row in cursor.fetchall():
            print(row[0])
        conn.close()
        return
    
    if not args.command:
        print("Error: command required (index or search) unless using -l or -k")
        sys.exit(1)
    
    if args.command == 'index':
        if not args.dir:
            print("Error: -d/--dir is required for index command")
            sys.exit(1)
        
        if os.path.exists(db_path):
            if args.overwrite and args.force:
                os.remove(db_path)
                print(f"Overwriting existing index: {db_path}")
            elif args.append:
                print(f"Appending to existing index: {db_path}")
            else:
                print(f"Index {args.dataset_name} already exists. Use -a to 'append' and '--overwrite --force' to overwrite.")
                sys.exit(1)
        
        indexer = Indexer(db_path, commit_time=args.commit_time, threads=args.threads, max_readahead_pct=args.max_readahead_sysmem)
        indexer.verbosity = args.verbose
        indexer.index_directory(args.dir, args.maxsize, args.append, args.recursive, args.maxindexed)
        print(f"Indexing complete. Database: {db_path}")
    
    elif args.command == 'search':
        if not args.query:
            print("Error: search query required")
            sys.exit(1)
        
        query_str = ' '.join(args.query)
            
        searcher = Searcher(db_path)
        
        # Determine query type
        if '<' in query_str:
            query_str = query_str.replace('<', ' ')
            results = searcher.sequential_search(query_str, debug=args.debug)
        elif any(op in query_str for op in ['&&', '||', '!', '(', ')']):
            results = searcher.boolean_search(query_str, debug=args.debug)
        elif len(args.query) > 1 or ' ' in query_str:
            results = searcher.sequential_search(query_str, debug=args.debug)
        else:
            results = searcher.boolean_search(query_str, debug=args.debug)
        
        if not results:
            print("No matches found")
            return
        
        # Extract keywords for highlighting
        highlight_keywords = []
        query_str = ' '.join(args.query)
        
        if '<' in query_str or len(args.query) > 1 or (' ' in query_str and not any(op in query_str for op in ['&&', '||', '!', '(', ')'])):
            highlight_keywords = [kw.strip().lower() for kw in query_str.replace('<', ' ').split() if kw.strip()]
        else:
            tokens = re.findall(r'!?\w+', query_str)
            for token in tokens:
                if token.lower() not in ('and', 'or', 'not') and not token.startswith('!'):
                    highlight_keywords.append(token.lower())
        
        # Display results
        for result in results:
            filename = result[0]
            keyword = result[1]
            offset = result[2]
            prior = result[3]
            match_line = result[4]
            next_line = result[5]
            
            print(f"\n{filename}:{offset} ({keyword})")
            if prior:
                print(f"  {searcher.highlight_keywords(prior, highlight_keywords)}")
            print(f"→ {searcher.highlight_keywords(match_line, highlight_keywords)}")
            if next_line:
                print(f"  {searcher.highlight_keywords(next_line, highlight_keywords)}")


if __name__ == '__main__':
    main()
