#!/usr/bin/env python3
"""
indexed-find: Fast keyword-based file search with boolean and sequential queries
"""

import sqlite3
import re
import os
import sys
import time
import threading
import queue
import psutil
from pathlib import Path
from typing import List, Tuple, Set, Optional
import argparse
from bansi import *

class BooleanParser:
    """Parse and evaluate boolean expressions"""
    
    def __init__(self, keywords: Set[str]):
        self.keywords = keywords
    
    def parse(self, expr: str) -> bool:
        """Parse boolean expression with &&, ||, (), and ! operators"""
        expr = expr.strip()
        
        # Handle parentheses recursively
        while '(' in expr:
            # Find innermost parentheses
            start = expr.rfind('(')
            end = expr.find(')', start)
            if end == -1:
                raise ValueError("Mismatched parentheses")
            
            inner = expr[start+1:end]
            result = self.parse(inner)
            expr = expr[:start] + str(result) + expr[end+1:]
        
        # Handle OR (lower precedence)
        if '||' in expr:
            parts = expr.split('||')
            return any(self.parse(p.strip()) for p in parts)
        
        # Handle AND (higher precedence)
        if '&&' in expr:
            parts = expr.split('&&')
            return all(self.parse(p.strip()) for p in parts)
        
        # Handle NOT
        if expr.startswith('!'):
            return not self.parse(expr[1:].strip())
        
        # Base case: check if keyword exists
        if expr in ('True', 'False'):
            return expr == 'True'
        
        return expr.strip().lower() in self.keywords


class Indexer:
    """Index files for fast keyword search"""
    
    def __init__(self, db_path: str, commit_time: float = 0, threads: int = 1, max_readahead_pct: float = 10):
        self.db_path = db_path
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        
        # Performance optimizations for SMR drives
        self.conn.execute('PRAGMA journal_mode=WAL')
        self.conn.execute('PRAGMA synchronous=NORMAL')
        self.conn.execute('PRAGMA cache_size=-4000000')  # 4GB cache
        self.conn.execute('PRAGMA temp_store=MEMORY')
        self.conn.execute('PRAGMA mmap_size=268435456')  # 256MB mmap
        
        self.create_tables()
        self.stats = {'total': 0, 'indexed': 0, 'skipped': 0, 'bytes': 0}
        self.verbosity = 0
        self.commit_time = commit_time
        self.last_commit = time.time()
        self.threads = threads
        self.max_readahead_bytes = int(psutil.virtual_memory().total * (max_readahead_pct / 100))
        self.stats_lock = threading.Lock()
        self.start_time = None

    def finalize_indexes(self):
        """Call after all indexing complete"""
        print("Building indexes (this may take a while on SMR drives)...")
        self.conn.execute('CREATE INDEX IF NOT EXISTS idx_keyword ON keywords(keyword)')
        self.conn.execute('CREATE INDEX IF NOT EXISTS idx_filename ON keywords(filename)')
        self.conn.commit()
    
    def create_tables(self):
        """Create database schema"""
        self.conn.execute('PRAGMA journal_mode=WAL')
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS keywords (
                id INTEGER PRIMARY KEY,
                keyword TEXT NOT NULL,
                filename TEXT NOT NULL,
                offset INTEGER NOT NULL,
                prior_line TEXT,
                match_line TEXT,
                next_line TEXT
            )
        ''')
        self.conn.execute('CREATE INDEX IF NOT EXISTS idx_keyword ON keywords(keyword)')
        self.conn.execute('CREATE INDEX IF NOT EXISTS idx_filename ON keywords(filename)')
        self.conn.commit()
    
    def is_binary_file(self, content: bytes, sample_size: int = 8192) -> bool:
        """
        Detect if file content is binary by checking for null bytes
        and high proportion of non-text bytes in sample
        """
        sample = content[:sample_size]
        
        # Quick check: null bytes are strong indicator of binary
        if b'\x00' in sample:
            return True
        
        # Check for high proportion of non-printable characters
        # Allow common whitespace: \t \n \r
        text_chars = bytearray({7, 8, 9, 10, 12, 13, 27} | set(range(0x20, 0x100)))
        non_text = sum(1 for byte in sample if byte not in text_chars)
        
        # If more than 30% non-text characters, likely binary
        if len(sample) > 0 and (non_text / len(sample)) > 0.3:
            return True
        
        return False
    
    def extract_keywords(self, text: str) -> Set[str]:
        """Extract keywords using both patterns"""
        keywords = set()
        
        # Pattern 1: [a-z0-9]+
        for match in re.finditer(r'[a-z0-9]+', text, re.IGNORECASE):
            kw = match.group(0).lower()
            if len(kw) <= 40:
                keywords.add(kw)
        
        # Pattern 2: [a-z0-9_]+
        for match in re.finditer(r'[a-z0-9_]+', text, re.IGNORECASE):
            kw = match.group(0).lower()
            if len(kw) <= 40:
                keywords.add(kw)
        
        return keywords
    
    def get_context(self, content: bytes, offset: int) -> Tuple[str, str, str]:
        """Extract prior, current, and next line with 160 char limits"""
        try:
            text = content.decode('utf-8', errors='ignore')
        except:
            return '', '', ''
        
        # Find line boundaries
        line_start = text.rfind('\n', 0, offset)
        if line_start == -1:
            line_start = 0
        else:
            line_start += 1
        
        line_end = text.find('\n', offset)
        if line_end == -1:
            line_end = len(text)
        
        current_line = text[line_start:line_end]
        
        # Prior line
        prior_line = ''
        if line_start > 0:
            prior_start = text.rfind('\n', 0, line_start - 1)
            if prior_start == -1:
                prior_start = 0
            else:
                prior_start += 1
            
            prior_text = text[prior_start:line_start - 1]
            if len(prior_text) > 160:
                prior_line = prior_text[-160:]
            else:
                prior_line = prior_text
        
        # Next line
        next_line = ''
        if line_end < len(text) - 1:
            next_start = line_end + 1
            next_end = text.find('\n', next_start)
            if next_end == -1:
                next_end = len(text)
            
            next_text = text[next_start:next_end]
            if len(next_text) > 160:
                next_line = next_text[:160]
            else:
                next_line = next_text
        
        # Limit current line
        if len(current_line) > 160:
            # Find where match is in current line
            rel_offset = offset - line_start
            if rel_offset < 160:
                current_line = current_line[:160]
            else:
                current_line = current_line[rel_offset-80:rel_offset+80]
        
        return prior_line, current_line, next_line
    
    def maybe_commit(self):
        """Commit if enough time has passed since last commit"""
        now = time.time()
        if self.commit_time == 0 or (now - self.last_commit) >= self.commit_time:
            self.conn.commit()
            self.last_commit = now
    
    def is_indexed(self, filepath: str) -> bool:
        """Check if file is already indexed"""
        cursor = self.conn.execute('''
            SELECT COUNT(*) FROM keywords WHERE filename = ?
        ''', (filepath,))
        return cursor.fetchone()[0] > 0
    
    def format_stats(self):
        """Format stats with rates"""
        if not self.start_time:
            return f"Total:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']}"
        
        elapsed = time.time() - self.start_time
        if elapsed < 0.1:
            return f"Total:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']}"
        
        files_per_sec = self.stats['indexed'] / elapsed
        mb_per_sec = (self.stats['bytes'] / elapsed) / (1024 * 1024)
        
        return f"Total:{self.stats['total']}, Indexed:{self.stats['indexed']}, Skipped:{self.stats['skipped']}, {files_per_sec:.1f} files/s, {mb_per_sec:.2f} MB/s"
    
    def index_directory(self, directory: str, maxsize: int = 200000, append: bool = False, recursive: bool = False, maxindexed: int = None):
        """Index files in directory (non-recursive by default, processes one at a time)"""
        self.start_time = time.time()
        
        if self.threads == 1:
            self._index_directory_single_threaded(directory, maxsize, append, recursive, maxindexed)
        else:
            self._index_directory_multi_threaded(directory, maxsize, append, recursive, maxindexed)
    
    def _index_directory_single_threaded(self, directory: str, maxsize: int, append: bool, recursive: bool, maxindexed: int):
        """Original single-threaded implementation"""
        base_dir = os.path.abspath(directory)
        
        try:
            if recursive:
                for root, dirs, files in os.walk(base_dir):
                    for filename in files:
                        if maxindexed and self.stats['indexed'] >= maxindexed:
                            print(f"\nReached max indexed limit: {maxindexed}")
                            self.conn.commit()
                            return
                        filepath = os.path.join(root, filename)
                        relative_path = os.path.relpath(filepath, base_dir)
                        self._index_file_if_valid(filepath, relative_path, maxsize, append)
            else:
                try:
                    entries = os.listdir(base_dir)
                except OSError as e:
                    print(f"Error reading directory {base_dir}: {e}", file=sys.stderr)
                    return
                
                for entry in entries:
                    if maxindexed and self.stats['indexed'] >= maxindexed:
                        print(f"\nReached max indexed limit: {maxindexed}")
                        self.conn.commit()
                        return
                    filepath = os.path.join(base_dir, entry)
                    if os.path.isfile(filepath):
                        self._index_file_if_valid(filepath, entry, maxsize, append)
        
        except KeyboardInterrupt:
            print(f"\n\nCtrl-C detected! Committing indexed data...")
            self.conn.commit()
            print(f"Graceful shutdown complete. {self.format_stats()}")
            sys.exit(0)
        
        self.conn.commit()
        if self.verbosity == 0:
            print(f"\r{self.format_stats()}")
        else:
            print(f"\r{self.format_stats()}")

    def _index_directory_multi_threaded(self, directory: str, maxsize: int, append: bool, recursive: bool, maxindexed: int):
        """Multi-threaded implementation with reader, workers, and writer"""
        base_dir = os.path.abspath(directory)
        
        # Queues
        work_queue = queue.Queue(maxsize=100)
        write_queue = queue.Queue(maxsize=5000)
        
        # Shared state
        reader_done = threading.Event()
        workers_done = threading.Event()
        shutdown = threading.Event()
        queue_bytes_lock = threading.Lock()
        queued_bytes = [0]
        
        # Reader thread
        def reader():
            try:
                file_list = []
                if recursive:
                    for root, dirs, files in os.walk(base_dir):
                        for filename in files:
                            filepath = os.path.join(root, filename)
                            relative_path = os.path.relpath(filepath, base_dir)
                            file_list.append((filepath, relative_path))
                else:
                    try:
                        entries = os.listdir(base_dir)
                        for entry in entries:
                            filepath = os.path.join(base_dir, entry)
                            if os.path.isfile(filepath):
                                file_list.append((filepath, entry))
                    except OSError as e:
                        print(f"Error reading directory: {e}", file=sys.stderr)
                        return
                
                for filepath, relative_path in file_list:
                    if shutdown.is_set():
                        if self.verbosity >= 1:
                            print("Reader: Shutdown requested, stopping")
                        break
                    
                    with self.stats_lock:
                        if maxindexed and self.stats['indexed'] >= maxindexed:
                            break
                    
                    try:
                        file_size = os.path.getsize(filepath)
                    except OSError:
                        with self.stats_lock:
                            self.stats['total'] += 1
                            self.stats['skipped'] += 1
                        continue
                    
                    if file_size == 0 or file_size > maxsize:
                        with self.stats_lock:
                            self.stats['total'] += 1
                            self.stats['skipped'] += 1
                        if self.verbosity >= 2 and file_size > maxsize:
                            print(f"Skipping {relative_path} (size: {file_size} > {maxsize})")
                        continue
                    
                    if append and self.is_indexed(relative_path):
                        with self.stats_lock:
                            self.stats['total'] += 1
                            self.stats['skipped'] += 1
                        if self.verbosity >= 2:
                            print(f"Skipping {relative_path} (already indexed)")
                        continue
                    
                    # Memory limit check
                    with queue_bytes_lock:
                        while queued_bytes[0] > self.max_readahead_bytes and queued_bytes[0] > 0:
                            queue_bytes_lock.release()
                            if shutdown.is_set():
                                if self.verbosity >= 1:
                                    print("Reader: Shutdown during queue wait")
                                return
                            time.sleep(0.01)
                            queue_bytes_lock.acquire()
                    
                    try:
                        with open(filepath, 'rb') as f:
                            content = f.read()
                        
                        with queue_bytes_lock:
                            queued_bytes[0] += len(content)
                        
                        work_queue.put((filepath, content, relative_path, len(content)))
                        
                        with self.stats_lock:
                            self.stats['total'] += 1
                            if self.verbosity >= 1:
                                print(f"\r{self.format_stats()} Reading: {relative_path} ({file_size} bytes)")
                        
                        if self.verbosity >= 1:
                            print(f"Reading: {relative_path} ({file_size} bytes)")
                            
                    except Exception as e:
                        print(f"Error reading {filepath}: {e}", file=sys.stderr)
                        with self.stats_lock:
                            self.stats['total'] += 1
                            self.stats['skipped'] += 1
            
            finally:
                reader_done.set()
                if self.verbosity >= 1:
                    print("Reader: Complete")
        
        # Worker threads
        def worker():
            while not (reader_done.is_set() and work_queue.empty()):
                if shutdown.is_set() and work_queue.empty():
                    if self.verbosity >= 1:
                        print(f"{threading.current_thread().name}: Shutdown, queue empty")
                    break
                
                try:
                    filepath, content, relative_path, file_bytes = work_queue.get(timeout=0.1)
                    
                    try:
                        # Check if binary BEFORE trying to decode
                        if self.is_binary_file(content):
                            if self.verbosity >= 2:
                                print(f"Skipping {relative_path} (binary file)")
                            with self.stats_lock:
                                self.stats['skipped'] += 1
                            continue
                        
                        text = content.decode('utf-8', errors='strict')
                        
                        if self.verbosity >= 2:
                            print(f"Indexing: {relative_path}")
                        
                        for pattern in [r'[a-z0-9]+', r'[a-z0-9_]+']:
                            for match in re.finditer(pattern, text, re.IGNORECASE):
                                keyword = match.group(0).lower()
                                if 3 <= len(keyword) <= 40:
                                    offset = match.start()
                                    prior, current, next_line = self.get_context(content, offset)
                                    
                                    write_queue.put((keyword, relative_path, offset, prior, current, next_line))
                        
                        with self.stats_lock:
                            self.stats['indexed'] += 1
                            self.stats['bytes'] += file_bytes
                            if self.verbosity >= 1:
                                print(f"\r{self.format_stats()} Indexed: {relative_path}")
                    
                    except UnicodeDecodeError:
                        if self.verbosity >= 2:
                            print(f"Skipping {relative_path} (invalid UTF-8)")
                        with self.stats_lock:
                            self.stats['skipped'] += 1
                    except Exception as e:
                        print(f"Error processing {filepath}: {e}", file=sys.stderr)
                        with self.stats_lock:
                            self.stats['skipped'] += 1
                    
                    finally:
                        with queue_bytes_lock:
                            queued_bytes[0] -= file_bytes
                        work_queue.task_done()
                
                except queue.Empty:
                    continue
            
            if self.verbosity >= 1:
                print(f"{threading.current_thread().name}: Complete")
        
        # Writer thread
        def writer():
            batch = []
            last_flush = time.time()
            
            while not (workers_done.is_set() and write_queue.empty()):
                if shutdown.is_set() and write_queue.empty():
                    if self.verbosity >= 1:
                        print("Writer: Shutdown, queue empty")
                    break
                
                try:
                    item = write_queue.get(timeout=0.1)
                    batch.append(item)
                    
                    now = time.time()
                    if len(batch) >= 100000 or (self.commit_time > 0 and (now - last_flush) >= self.commit_time):
                        commit_start = time.time()
                        if self.verbosity >= 2:
                            print(f"Storing Index: {len(batch)} keywords")
                        self.conn.executemany('''
                            INSERT INTO keywords (keyword, filename, offset, prior_line, match_line, next_line)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', batch)
                        self.conn.commit()
                        commit_time = time.time() - commit_start
                        if self.verbosity >= 1:
                            print(f"Commit took {bmag}{commit_time:.2f}{rst}s")
                        batch = []
                        last_flush = now
                    
                    write_queue.task_done()
                
                except queue.Empty:
                    if batch:
                        now = time.time()
                        if self.commit_time == 0 or (now - last_flush) >= self.commit_time:
                            commit_start = time.time()
                            if self.verbosity >= 2:
                                print(f"Storing Index: {len(batch)} keywords")
                            self.conn.executemany('''
                                INSERT INTO keywords (keyword, filename, offset, prior_line, match_line, next_line)
                                VALUES (?, ?, ?, ?, ?, ?)
                            ''', batch)
                            self.conn.commit()
                            commit_time = time.time() - commit_start
                            if self.verbosity >= 1:
                                print(f"Commit took {commit_time:.2f}s")
                            batch = []
                            last_flush = now
            
            # Final commit
            if batch:
                commit_start = time.time()
                if self.verbosity >= 2:
                    print(f"Storing Index: {len(batch)} keywords (final)")
                self.conn.executemany('''
                    INSERT INTO keywords (keyword, filename, offset, prior_line, match_line, next_line)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', batch)
                self.conn.commit()
                commit_time = time.time() - commit_start
                if self.verbosity >= 1:
                    print(f"Final commit took {commit_time:.2f}s")
            
            if self.verbosity >= 1:
                print("Writer: Complete")
        
        # Start threads
        reader_thread = threading.Thread(target=reader, name="Reader")
        worker_threads = [threading.Thread(target=worker, name=f"Worker-{i}") 
                         for i in range(max(1, self.threads - 2))]
        writer_thread = threading.Thread(target=writer, name="Writer")
        
        reader_thread.start()
        for t in worker_threads:
            t.start()
        writer_thread.start()
        
        start_time = time.time()
        try:
            while reader_thread.is_alive() or any(t.is_alive() for t in worker_threads) or writer_thread.is_alive():
                if self.verbosity == 0:
                    with self.stats_lock:
                        print(f"\r{self.format_stats()}", end='', flush=True)
                time.sleep(0.5)
        except KeyboardInterrupt:
            print("\n\nCtrl-C detected! Signaling threads to stop gracefully...")
            shutdown.set()
            
            # Give threads time to finish current work
            print("Waiting for reader to stop...")
            reader_thread.join()
            
            print("Waiting for workers to finish current files...")
            for t in worker_threads:
                t.join()
            
            # Signal that all workers are done
            workers_done.set()
            
            print("Waiting for writer to commit remaining data (may take a while)...")
            writer_thread.join()
            
            elapsed = time.time() - start_time
            print(f"\nGraceful shutdown complete. {self.format_stats()} (ran for {elapsed:.1f}s)")
            sys.exit(0)
        
        reader_thread.join()
        for t in worker_threads:
            t.join()
        
        # Signal that all workers are done NOW (after they all finish)
        workers_done.set()
        
        writer_thread.join()
        
        elapsed = time.time() - start_time
        if self.verbosity == 0:
            print(f"\r{self.format_stats()} (took {elapsed:.1f}s)")
        else:
            print(f"{self.format_stats()} (took {elapsed:.1f}s)")
    
    def _index_file_if_valid(self, filepath: str, relative_path: str, maxsize: int, append: bool):
        """Helper to check and index a single file"""
        self.stats['total'] += 1
        
        if self.verbosity == 0:
            print(f"\r{self.format_stats()}", end='', flush=True)
        else:
            print(f"{self.format_stats()}", end=' ', flush=True)
        
        try:
            file_size = os.path.getsize(filepath)
        except OSError:
            self.stats['skipped'] += 1
            if self.verbosity >= 2:
                print(f"Skipping {relative_path} (error reading)")
            elif self.verbosity >= 1:
                print()
            return
        
        if file_size == 0:
            self.stats['skipped'] += 1
            if self.verbosity >= 2:
                print(f"Skipping {relative_path} (0 bytes)")
            elif self.verbosity >= 1:
                print()
            return
        
        if file_size > maxsize:
            self.stats['skipped'] += 1
            if self.verbosity >= 2:
                print(f"Skipping {relative_path} (size: {file_size} > {maxsize})")
            elif self.verbosity >= 1:
                print()
            return
        
        if append and self.is_indexed(relative_path):
            self.stats['skipped'] += 1
            if self.verbosity >= 2:
                print(f"Skipping {relative_path} (already indexed)")
            elif self.verbosity >= 1:
                print()
            return
        
        if self.verbosity >= 1:
            print(f"Indexing: {relative_path} ({file_size} bytes)")
        
        self.stats['indexed'] += 1
        self.stats['bytes'] += file_size
        self.index_file_with_name(filepath, relative_path)
    
    def index_file(self, filepath: str):
        """Index a single file (deprecated - use index_file_with_name)"""
        self.index_file_with_name(filepath, filepath)
    
    def index_file_with_name(self, filepath: str, stored_name: str):
        """Index a file but store it with the given name"""
        try:
            with open(filepath, 'rb') as f:
                content = f.read()
            
            # Check if binary
            if self.is_binary_file(content):
                if self.verbosity >= 2:
                    print(f"Skipping {stored_name} (binary file)")
                return
            
            text = content.decode('utf-8', errors='strict')
            
            for pattern in [r'[a-z0-9]+', r'[a-z0-9_]+']:
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    keyword = match.group(0).lower()
                    if 3 <= len(keyword) <= 40:
                        offset = match.start()
                        prior, current, next_line = self.get_context(content, offset)
                        
                        self.conn.execute('''
                            INSERT INTO keywords (keyword, filename, offset, prior_line, match_line, next_line)
                            VALUES (?, ?, ?, ?, ?, ?)
                        ''', (keyword, stored_name, offset, prior, current, next_line))
            
            self.maybe_commit()
        except UnicodeDecodeError:
            if self.verbosity >= 2:
                print(f"Skipping {stored_name} (invalid UTF-8)")
        except Exception as e:
            print(f"Error indexing {filepath}: {e}", file=sys.stderr)


class Searcher:
    """Search indexed files"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.HIGHLIGHT = '\033[35;1m'
        self.RESET = '\033[0m'
    
    def highlight_keywords(self, text: str, keywords: List[str]) -> str:
        """Highlight keywords in text"""
        if not text:
            return text
        
        result = text
        sorted_keywords = sorted(keywords, key=len, reverse=True)
        
        for keyword in sorted_keywords:
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            result = pattern.sub(f'{self.HIGHLIGHT}\\g<0>{self.RESET}', result)
        
        return result
    
    def boolean_search(self, query: str, debug: bool = False) -> List[Tuple]:
        """Search using boolean logic"""
        potential_keywords = re.findall(r'\w+', query)
        keywords_to_check = [kw.lower() for kw in potential_keywords 
                            if kw.lower() not in ('and', 'or', 'not')]
        
        if debug:
            print(f"Debug: Searching for keywords: {keywords_to_check}", file=sys.stderr)
        
        if not keywords_to_check:
            return []
        
        placeholders = ','.join('?' * len(keywords_to_check))
        cursor = self.conn.execute(f'''
            SELECT DISTINCT filename FROM keywords WHERE keyword IN ({placeholders})
        ''', keywords_to_check)
        
        files = [row[0] for row in cursor.fetchall()]
        
        if debug:
            print(f"Debug: Found {len(files)} files with at least one keyword", file=sys.stderr)
        
        results = []
        for filename in files:
            cursor = self.conn.execute('''
                SELECT DISTINCT keyword FROM keywords WHERE filename = ?
            ''', (filename,))
            file_keywords = set(row[0] for row in cursor.fetchall())
            
            if debug:
                matching = [kw for kw in keywords_to_check if kw in file_keywords]
                print(f"Debug: {filename} has keywords: {matching}", file=sys.stderr)
            
            parser = BooleanParser(file_keywords)
            try:
                if parser.parse(query):
                    if debug:
                        print(f"Debug: {filename} MATCHES query", file=sys.stderr)
                    
                    positive_keywords = [kw for kw in keywords_to_check if not query.count(f'!{kw}')]
                    if not positive_keywords:
                        positive_keywords = keywords_to_check
                    
                    placeholders_pos = ','.join('?' * len(positive_keywords))
                    cursor = self.conn.execute(f'''
                        SELECT keyword, offset, prior_line, match_line, next_line
                        FROM keywords WHERE filename = ? AND keyword IN ({placeholders_pos})
                        LIMIT 3
                    ''', (filename,) + tuple(positive_keywords))
                    
                    for row in cursor.fetchall():
                        results.append((filename,) + row)
            except Exception as e:
                print(f"Error parsing query: {e}", file=sys.stderr)
                return []
        
        return results
    
    def sequential_search(self, query: str, debug: bool = False) -> List[Tuple]:
        """Search for keywords in order using space-separated or < operator"""
        keywords = [kw.strip().lower() for kw in query.split() if kw.strip()]
        
        if debug:
            print(f"Debug: Sequential search for: {keywords}", file=sys.stderr)
        
        if len(keywords) < 2:
            return []
        
        placeholders = ','.join('?' * len(keywords))
        cursor = self.conn.execute(f'''
            SELECT filename FROM keywords WHERE keyword IN ({placeholders})
            GROUP BY filename HAVING COUNT(DISTINCT keyword) = ?
        ''', keywords + [len(keywords)])
        
        files = [row[0] for row in cursor.fetchall()]
        
        if debug:
            print(f"Debug: Found {len(files)} files with all keywords", file=sys.stderr)
        
        results = []
        for filename in files:
            cursor = self.conn.execute(f'''
                SELECT keyword, offset, prior_line, match_line, next_line
                FROM keywords WHERE filename = ? AND keyword IN ({placeholders})
                ORDER BY offset
            ''', (filename,) + tuple(keywords))
            
            occurrences = cursor.fetchall()
            
            keyword_positions = {kw: [] for kw in keywords}
            for kw, offset, *context in occurrences:
                keyword_positions[kw].append((offset, context))
            
            if self.check_sequence(keyword_positions, keywords):
                if debug:
                    print(f"Debug: {filename} has keywords in correct sequence", file=sys.stderr)
                
                for kw in keywords:
                    if keyword_positions[kw]:
                        offset, context = keyword_positions[kw][0]
                        results.append((filename, kw, offset) + tuple(context))
        
        return results
    
    def check_sequence(self, positions: dict, keywords: List[str]) -> bool:
        """Check if keywords appear in sequence"""
        for first_offset, _ in positions[keywords[0]]:
            last_offset = first_offset
            valid = True
            
            for kw in keywords[1:]:
                found = False
                for offset, _ in positions[kw]:
                    if offset > last_offset:
                        last_offset = offset
                        found = True
                        break
                
                if not found:
                    valid = False
                    break
            
            if valid:
                return True
        
        return False


def main():
    DEF_MAXSIZE = 200000
    DEF_COMMIT_TIME = 0
    DEF_THREADS = 2
    DEF_READAHEAD_PCT = 10
    
    parser = argparse.ArgumentParser(description='Index and search files for recovery')
    parser.add_argument('-n', '--dataset-name', required=True, help='Dataset name')
    parser.add_argument('command', nargs='?', choices=['index', 'search', 'finalize'], help='Command to run')
    parser.add_argument('query', nargs='*', help='Search query (multiple args for sequential search)')
    parser.add_argument('-d', '--dir', help='Directory to index (required for index command)')
    parser.add_argument('-r', '--recursive', action='store_true', help='Recursively index subdirectories')
    parser.add_argument('-a', '--append', action='store_true', help='Append to existing index')
    parser.add_argument('--overwrite', action='store_true', help='Overwrite existing index (requires --force)')
    parser.add_argument('--force', action='store_true', help='Force overwrite (requires --overwrite)')
    parser.add_argument('--maxsize', type=int, default=DEF_MAXSIZE, 
                       help=f'Maximum file size in bytes (default: {DEF_MAXSIZE})')
    parser.add_argument('-v', '--verbose', action='count', default=0,
                       help='Increase verbosity (-v, -v -v, or -v -v -v)')
    parser.add_argument('--maxindexed', type=int, default=None,
                       help='Stop after indexing N files (for testing)')
    parser.add_argument('--debug', action='store_true',
                       help='Show debug information for searches')
    parser.add_argument('-l', '--list-files', action='store_true',
                       help='List all files in the index')
    parser.add_argument('-k', '--list-keywords', action='store_true',
                       help='List all keywords in the index')
    parser.add_argument('--commit-time', '--ct', type=float, default=DEF_COMMIT_TIME,
                       help=f'Minimum time between commits to sqlite database. Default ({DEF_COMMIT_TIME}) = Write after each file is indexed.')
    parser.add_argument('--threads', type=int, default=DEF_THREADS,
                       help=f'Number of threads (default: {DEF_THREADS} = single-threaded). N > 1: Uses 1 reader + (N-1) workers + 1 writer')
    parser.add_argument('--max-readahead-sysmem', type=float, default=DEF_READAHEAD_PCT,
                       help=f'Percentage of system memory for file readahead queue (default: {DEF_READAHEAD_PCT}%%)')
    
    args = parser.parse_args()
    
    db_path = os.path.expanduser(f'~/.cache/indexor/ds/{args.dataset_name}/index.db')
    
    if args.list_files:
        if not os.path.exists(db_path):
            print(f"Index {args.dataset_name} does not exist")
            sys.exit(1)
        conn = sqlite3.connect(db_path)
        cursor = conn.execute('SELECT DISTINCT filename FROM keywords ORDER BY filename')
        for row in cursor.fetchall():
            print(row[0])
        conn.close()
        return
    
    if args.list_keywords:
        if not os.path.exists(db_path):
            print(f"Index {args.dataset_name} does not exist")
            sys.exit(1)
        conn = sqlite3.connect(db_path)
        cursor = conn.execute('SELECT DISTINCT keyword FROM keywords ORDER BY keyword')
        for row in cursor.fetchall():
            print(row[0])
        conn.close()
        return
    
    if not args.command:
        print("Error: command required (index, search, or finalize) unless using -l or -k")
        sys.exit(1)
    
    if args.command == 'finalize':
        if not os.path.exists(db_path):
            print(f"Error: Index {args.dataset_name} does not exist")
            sys.exit(1)
        
        print(f"Finalizing index: {db_path}")
        indexer = Indexer(db_path)
        indexer.finalize_indexes()
        print("Finalization complete.")
        return
    
    if args.command == 'index':
        if not args.dir:
            print("Error: -d/--dir is required for index command")
            sys.exit(1)
        
        if os.path.exists(db_path):
            if args.overwrite and args.force:
                os.remove(db_path)
                print(f"Overwriting existing index: {db_path}")
            elif args.append:
                print(f"Appending to existing index: {db_path}")
            else:
                print(f"Index {args.dataset_name} already exists. Use -a to 'append' and '--overwrite --force' to overwrite.")
                sys.exit(1)
        
        indexer = Indexer(db_path, commit_time=args.commit_time, threads=args.threads, max_readahead_pct=args.max_readahead_sysmem)
        indexer.verbosity = args.verbose
        indexer.index_directory(args.dir, args.maxsize, args.append, args.recursive, args.maxindexed)
        print(f"Indexing complete. Database: {db_path}")
    
    elif args.command == 'search':
        if not args.query:
            print("Error: search query required")
            sys.exit(1)
        
        query_str = ' '.join(args.query)
            
        searcher = Searcher(db_path)
        
        # Determine query type
        if '<' in query_str:
            query_str = query_str.replace('<', ' ')
            results = searcher.sequential_search(query_str, debug=args.debug)
        elif any(op in query_str for op in ['&&', '||', '!', '(', ')']):
            results = searcher.boolean_search(query_str, debug=args.debug)
        elif len(args.query) > 1 or ' ' in query_str:
            results = searcher.sequential_search(query_str, debug=args.debug)
        else:
            results = searcher.boolean_search(query_str, debug=args.debug)
        
        if not results:
            print("No matches found")
            return
        
        # Extract keywords for highlighting
        highlight_keywords = []
        query_str = ' '.join(args.query)
        
        if '<' in query_str or len(args.query) > 1 or (' ' in query_str and not any(op in query_str for op in ['&&', '||', '!', '(', ')'])):
            highlight_keywords = [kw.strip().lower() for kw in query_str.replace('<', ' ').split() if kw.strip()]
        else:
            tokens = re.findall(r'!?\w+', query_str)
            for token in tokens:
                if token.lower() not in ('and', 'or', 'not') and not token.startswith('!'):
                    highlight_keywords.append(token.lower())
        
        # Display results
        for result in results:
            filename = result[0]
            keyword = result[1]
            offset = result[2]
            prior = result[3]
            match_line = result[4]
            next_line = result[5]
            
            print(f"\n{filename}:{offset} ({keyword})")
            if prior:
                print(f"  {searcher.highlight_keywords(prior, highlight_keywords)}")
            print(f"â†’ {searcher.highlight_keywords(match_line, highlight_keywords)}")
            if next_line:
                print(f"  {searcher.highlight_keywords(next_line, highlight_keywords)}")


if __name__ == '__main__':
    main()
